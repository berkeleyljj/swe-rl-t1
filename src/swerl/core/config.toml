# Default model to use
default_model = "QwQ-32B"

# Model configurations
[models.gpt-4]
name = "gpt-4"
provider = "openai"
api_key = "env:OPENAI_API_KEY"  # Will be loaded from environment variable
api_version = null  # Optional API version
custom_llm_provider = null  # Optional custom provider

# Model parameters
temperature = 0.0  # Temperature for non-reasoning models
max_new_tokens = 1024  # Maximum tokens in response
max_input_tokens = 8192  # Maximum tokens in input
batch_size = 1  # Number of completions to generate
top_p = 1.0  # Top-p sampling parameter
reasoning_effort = null  # For supported models only

# Retry configuration
num_retries = 3  # Number of retries on failure
retry_min_wait = 4  # Minimum wait time between retries
retry_max_wait = 10  # Maximum wait time between retries
retry_multiplier = 2  # Multiplier for exponential backoff
timeout = 100  # Request timeout in seconds

# Features
prompt_cache = false  # Enable prompt caching (supported models only)
stream = false  # Enable streaming responses
drop_params = true  # Drop unsupported parameters
modify_params = true  # Allow LiteLLM to modify parameters
stop = null  # Optional stop words

[models.claude-3]
name = "claude-3-sonnet-20240229"
provider = "anthropic"
api_key = "env:ANTHROPIC_API_KEY"
temperature = 0.0
max_new_tokens = 4096
max_input_tokens = 200000  # Claude-3 supports very long inputs
batch_size = 1
prompt_cache = true  # Claude-3 supports prompt caching
tools = [
    { type = "function", function = { name = "str_replace_editor", description = "Custom editing tool for editing files", parameters = { type = "object", properties = { path = { description = "Full path to file", type = "string" }, old_str = { description = "String to replace", type = "string" }, new_str = { description = "New string", type = "string" } }, required = ["path", "old_str"] } } }
]

[models.o3-mini]
name = "o3-mini"
provider = "openai"
api_key = "env:OPENAI_API_KEY"
temperature = null  # Not used with reasoning_effort
max_new_tokens = 4096
batch_size = 1
reasoning_effort = 0.5  # This model supports reasoning_effort
tools = []  # Supports function calling

[models.deepseek]
name = "deepseek-chat"
provider = "deepseek"
api_key = "env:DEEPSEEK_API_KEY"
base_url = "https://api.deepseek.com"
temperature = 0.0
max_new_tokens = 2048
top_p = 1.0

[models.QwQ-32B]
name = "QwQ-32B"
provider = "hosted_vllm/Qwen/QwQ-32B" 
base_url = "http://127.0.0.1:9000/v1/" 
api_key = "place_holder"  
temperature = 0.0
max_input_tokens = 4096
batch_size = 1
top_p = 1.0

# Logging configuration
[logging]
level = "INFO"
format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"